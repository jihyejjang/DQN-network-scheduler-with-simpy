{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random \n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque \n",
    "from itertools import product\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam \n",
    "\n",
    "class DQN:#모델 선언\n",
    "    def __init__(self): #parameter들의 초기값\n",
    "        self.epsilon = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.step = 1\n",
    "        self.tau = 0.125 #?\n",
    "        self.learning_rate = 1 #할인율. 1에 가까울 수록 미래에 받는 보상도 중요, 0에 가까울수록 즉각적인 보상이 중요\n",
    "        self.memory = deque()\n",
    "        self.model = self.create_model() #현재 state에 대한 model\n",
    "        self.target_model = self.create_model() #next state에 대한 model\n",
    "\n",
    "    # create the neural network to train the q function \n",
    "    def create_model(self): #Q값예측모델. \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim= 3, activation= 'relu')) # input dimension : source들 차원\n",
    "        model.add(Dense(48, activation= 'relu'))\n",
    "        model.add(Dense(24, activation= 'relu'))\n",
    "        model.add(Dense(66)) #계산했을 때,(1~10)까지 세 수의 합이 10이 되는 경우의수는 66개. output에 대한 가중치는 매번 update되기 때문에 이에 mapping시키면 된다. \n",
    "        model.compile(loss= 'mean_squared_error', optimizer= Adam(lr= 1e-3)) #optimizer의 learning rate도 주의\n",
    "        \n",
    "        return model \n",
    "\n",
    "\n",
    "\n",
    "    # Action function to choose the best action given the q-function if not exploring based on epsilon p값에 의한 예측이 아닐때\n",
    "    def choose_action(self, state, allowed_actions): #action을 선택 (parameter로 선택가능한 action이 들어옴)\n",
    "        select = False\n",
    "        if (self.step%10000 == 0):#약 10만번 step에서 3만번의 scheduling 발생. 10000번마다 epsilon이 감소\n",
    "            self.epsilon = max(self.epsilon_min, pow(self.epsilon,int(self.step/10000 +1)))\n",
    "        #print (\"epsilon\", self.epsilon)\n",
    "        self.step+=1\n",
    "        r = np.random.random()\n",
    "        if r < self.epsilon: #p값보다 작은 경우 랜덤한 액션을 취함\n",
    "            print(\"random action\")\n",
    "            return random.choice(allowed_actions),self.epsilon,select\n",
    "        \n",
    "        \n",
    "        print (\"@@action choose@@\" , self.step)\n",
    "        select = True\n",
    "        state = np.array(state).reshape(1,len(state)) #p값보다 큰경우, state 배열 생성\n",
    "        \n",
    "        pred = self.model.predict(state)[0]\n",
    "        #print (\"q\",pred)\n",
    "        \n",
    "        return self.maxQ_action(pred,allowed_actions),self.epsilon,select #Q예측값중 min_rate 이상으로 가장 큰 action을 선택\n",
    "    \n",
    "\n",
    "    def maxQ_action(self,pred,allowed_actions):#allowed action 생성 (min_rate 이상 조합만 남김)\n",
    "        print (\"max q\", np.argmax(pred) )\n",
    "        return allowed_actions[np.argmax(pred)]\n",
    "        \n",
    "        \n",
    "        \n",
    "    # create replay buffer memory to sample randomly #메모리에서 꺼내서 학습할 수 있게 저장, terminal이란 next_state가 없는 경우\n",
    "    def remember(self, state, action, reward, next_state,terminal):\n",
    "        self.memory.append([state, action, reward, next_state,terminal])\n",
    "\n",
    "\n",
    "    # build the replay buffer 저장한 것을 버퍼에서 꺼내오는.? 학습단계?\n",
    "    def replay(self,allowed_actions):\n",
    "        \n",
    "        #global mse_loss\n",
    "        mse=[]\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: #buffer에 저장된 memory가 buffer의 총 batch_size보다 작다면 return\n",
    "            return \n",
    "        \n",
    "        samples = random.sample(self.memory, batch_size) #메모리에서 배치사이즈만큼 랜덤으로 선택\n",
    "        \n",
    "        \n",
    "        for sample in samples:\n",
    "            \n",
    "            #print (\"sample\" , sample)\n",
    "            \n",
    "            state, action, reward, new_state, terminal = sample # sample 데이터 하나를 꺼내서\n",
    "            \n",
    "            state = np.array(state).reshape(1,len(state)) \n",
    "            \n",
    "            new_state = np.array(new_state).reshape(1,len(new_state))\n",
    "            \n",
    "            target = self.target_model.predict(state) \n",
    "            \n",
    "            action_id = allowed_actions.index(tuple(action)) #63개의 allowed action 중에서 state에 대한 action의 index를 추출\n",
    "\n",
    "            if terminal :\n",
    "                target[0][action_id] = reward\n",
    "            else :\n",
    "                next_pred = self.target_model.predict(new_state)[0] #new state에 대한 target 예측\n",
    "                  \n",
    "                Q_future= max(next_pred) #next state에 대한 predict 값 중 가장 큰 값이 Q값이 됨\n",
    "                \n",
    "                target[0][action_id] = reward + Q_future * self.learning_rate # target의 action_id번째 위치에 다음 Q값이 들어감. 맞춰야 하는 값!!!\n",
    "            \n",
    "            history=self.model.fit(state, target, epochs= 1, verbose= 0) \n",
    "            mse.append(history.history['loss'][0]) # loss 기록\n",
    "            \n",
    "        return min(mse)\n",
    "        \n",
    "        #print(\"Mean_square_error:\"min(mse_loss))\n",
    "        \n",
    "\n",
    "\n",
    "    # update our target network \n",
    "    def train_target(self): #target network를 업데이트\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)#loss함수?\n",
    "            #target_weights[i] = weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "\n",
    "\n",
    "    # save our model \n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
